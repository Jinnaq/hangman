{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trexquant Interview Project (The Hangman Game)\n",
    "\n",
    "* Copyright Trexquant Investment LP. All Rights Reserved. \n",
    "* Redistribution of this question without written consent from Trexquant is prohibited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction:\n",
    "For this coding test, your mission is to write an algorithm that plays the game of Hangman through our API server. \n",
    "\n",
    "When a user plays Hangman, the server first selects a secret word at random from a list. The server then returns a row of underscores (space separated)—one for each letter in the secret word—and asks the user to guess a letter. If the user guesses a letter that is in the word, the word is redisplayed with all instances of that letter shown in the correct positions, along with any letters correctly guessed on previous turns. If the letter does not appear in the word, the user is charged with an incorrect guess. The user keeps guessing letters until either (1) the user has correctly guessed all the letters in the word\n",
    "or (2) the user has made six incorrect guesses.\n",
    "\n",
    "You are required to write a \"guess\" function that takes current word (with underscores) as input and returns a guess letter. You will use the API codes below to play 1,000 Hangman games. You have the opportunity to practice before you want to start recording your game results.\n",
    "\n",
    "Your algorithm is permitted to use a training set of approximately 250,000 dictionary words. Your algorithm will be tested on an entirely disjoint set of 250,000 dictionary words. Please note that this means the words that you will ultimately be tested on do NOT appear in the dictionary that you are given. You are not permitted to use any dictionary other than the training dictionary we provided. This requirement will be strictly enforced by code review.\n",
    "\n",
    "You are provided with a basic, working algorithm. This algorithm will match the provided masked string (e.g. a _ _ l e) to all possible words in the dictionary, tabulate the frequency of letters appearing in these possible words, and then guess the letter with the highest frequency of appearence that has not already been guessed. If there are no remaining words that match then it will default back to the character frequency distribution of the entire dictionary.\n",
    "\n",
    "This benchmark strategy is successful approximately 18% of the time. Your task is to design an algorithm that significantly outperforms this benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hangman_api import *\n",
    "import model_jinna as model_jn\n",
    "import bidirectional_lstm as bi_lstm\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding,Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To start a new game:\n",
    "1. Make sure you have implemented your own \"guess\" method.\n",
    "2. Use the access_token that we sent you to create your HangmanAPI object. \n",
    "3. Start a game by calling \"start_game\" method.\n",
    "4. If you wish to test your function without being recorded, set \"practice\" parameter to 1.\n",
    "5. Note: You have a rate limit of 20 new games per minute. DO NOT start more than 20 new games within one minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "api = HangmanAPI(access_token=\"548234d05e76068d5ce791cbd1e644\", timeout=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design of the algorithm:\n",
    "# 1. For the first guess: select the letter with the highest occurence in all words with length +- 3 (lower bounded by 1)\n",
    "# 2. For the subsequent guesses: build and use n_gram dictionary to find the most probable letter \n",
    "# 3. check if LSTM is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: disembosom\n",
      "Chosen letter: 'm'\n",
      "Masked word:   dise_boso_\n"
     ]
    }
   ],
   "source": [
    "# LSTM: check the most probable guesses based on incomplete word\n",
    "words = api.full_dictionary  \n",
    "word_for_model = words[52423]\n",
    "def mask_random_letter(word):\n",
    "    if not word:\n",
    "        return word  # handle empty string\n",
    "\n",
    "    unique_letters = list(set(word))\n",
    "    chosen_letter = random.choice(unique_letters)\n",
    "    masked_word = word.replace(chosen_letter, '_')\n",
    "    \n",
    "    print(f\"Original word: {word}\")\n",
    "    print(f\"Chosen letter: '{chosen_letter}'\")\n",
    "    print(f\"Masked word:   {masked_word}\")\n",
    "    return masked_word\n",
    "\n",
    "# Example usage:\n",
    "masked = mask_random_letter(word_for_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('c', 0.4223), ('k', 0.4164), ('f', 0.0883), ('d', 0.0355), ('b', 0.0225), ('m', 0.0109), ('l', 0.0026), ('j', 0.0012), ('h', 0.0001), ('i', 0.0001), ('_', 0.0), ('a', 0.0), ('e', 0.0), ('g', 0.0), ('n', 0.0), ('o', 0.0), ('p', 0.0), ('q', 0.0), ('r', 0.0), ('s', 0.0), ('t', 0.0), ('u', 0.0), ('v', 0.0), ('w', 0.0), ('x', 0.0), ('y', 0.0), ('z', 0.0)]\n",
      "c: 0.4223\n",
      "k: 0.4164\n",
      "f: 0.0883\n",
      "d: 0.0355\n",
      "b: 0.0225\n",
      "m: 0.0109\n"
     ]
    }
   ],
   "source": [
    "# lstm_model.save('my_lstm_model.keras')  # newer Keras format\n",
    "# lstm_model = keras.models.load_model('my_lstm_model.keras')\n",
    "model = model_jn.LSTMCharModel(vocab_size = 27)\n",
    "model.load_state_dict(torch.load('lstm_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "import json\n",
    "with open(\"model_metadata.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract specific parts\n",
    "char_to_idx = data[\"char_to_idx\"]\n",
    "idx_to_char = {int(k): v for k, v in data[\"idx_to_char\"].items()}  # Convert keys to int\n",
    "max_len = data[\"max_len\"]\n",
    "\n",
    "ranked_predictions = model_jn.get_ranked_letter_probs(model, masked, char_to_idx, idx_to_char, max_len)\n",
    "print(ranked_predictions)\n",
    "\n",
    "# Show top 10\n",
    "for char, prob in ranked_predictions[:6]:\n",
    "    print(f\"{char}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions (char: prob):\n",
      "n: 0.9419\n",
      "p: 0.0316\n",
      "g: 0.0078\n",
      "k: 0.0045\n",
      "s: 0.0042\n",
      "z: 0.0042\n",
      "o: 0.0035\n"
     ]
    }
   ],
   "source": [
    "bi_lstm_model, vocab = bi_lstm.load_model()         \n",
    "# print('top 6 letters predicted by lstm are: ')\n",
    "most_common_by_model = bi_lstm.predict_missing(bi_lstm_model, vocab, masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments\n",
    "LSTM \n",
    "- one directional lstm works better in actually prediction \n",
    "- but still mostly perform worse than ngram \n",
    "- when it predicted correctly, also most of time aligned with the ngrams result \n",
    "- lots of time unable to find out the last letter \n",
    "\n",
    "Ngram \n",
    "- works much better and faster when I reduced min gram from 3 to len(word)-4 (reduce number of matched grams)\n",
    "- but the issues is the matched grams will quickly dropped to [] \n",
    "- too many matched case is not good \n",
    "- although there are matched grams but there is no nonguessed letters\n",
    "\n",
    "Next steps:\n",
    "1. [done] set a backup n_gram dictionary with smaller n (only use ngram)\n",
    "2. must we ensure matching lengthen when comparing n_gram? contain also works? especially for shorter words?\n",
    "    if word lengthen < x, we cut by x-1 and cut dictionary by x-1 to x+3 and use contain logics \n",
    "2. use pretrained language model to split the word into phonetic syllabuls and then use n_gram \n",
    "2. train another neural network model (not lstm) with inputs as such: word[:i], word[i+1:], masked_word, i, i/len() and output as word[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing practice games:\n",
    "You can use the command below to play up to 100,000 practice games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing  1  th game\n",
      "Successfully start a new game! Game ID: 01cbc242e26c. # of tries remaining: 6. Word: _ _ _ _ _ _ _ _ _ _ .\n",
      "Guessing letter: e\n",
      "Sever response: {'game_id': '01cbc242e26c', 'status': 'ongoing', 'tries_remains': 6, 'word': '_ _ e _ _ _ _ _ _ _ '}\n",
      "Guessing letter: r\n",
      "Sever response: {'game_id': '01cbc242e26c', 'status': 'ongoing', 'tries_remains': 6, 'word': '_ r e _ _ _ _ _ r _ '}\n",
      "len(self.n_gram_dictionary) is 2138152\n",
      "len(self.n_gram_backup) is 7468580\n",
      "len(n_gram_dictionary) is 2138152\n",
      "some matched grams are: \n",
      "['breviat', 'breviat', 'breviat', 'irepullers', 'dremembere', 'wreathwork']\n",
      "letters predicted by n_grams are: \n",
      "[('a', 48615), ('o', 48458), ('i', 46073), ('t', 42365), ('n', 32968), ('s', 32785), ('p', 25334), ('c', 25025), ('l', 22102), ('u', 20524), ('d', 17494), ('m', 15762), ('h', 14445), ('g', 12451), ('b', 11434), ('f', 9089), ('y', 7362), ('v', 6649), ('w', 4700), ('k', 3340), ('x', 1707), ('q', 969), ('z', 850), ('j', 673)]\n",
      "Guessing letter: a\n",
      "Sever response: {'game_id': '01cbc242e26c', 'status': 'ongoing', 'tries_remains': 6, 'word': '_ r e _ _ _ _ a r _ '}\n",
      "len(self.n_gram_dictionary) is 96775\n",
      "len(self.n_gram_backup) is 7468580\n",
      "len(n_gram_dictionary) is 96775\n",
      "some matched grams are: \n",
      "['breviat', 'breviat', 'breviat', 'presidiary', 'dreyfusard', 'hreesquare']\n",
      "letters predicted by n_grams are: \n",
      "[('i', 9832), ('t', 8525), ('p', 7665), ('n', 7243), ('o', 6873), ('s', 6687), ('c', 6192), ('l', 5265), ('d', 4750), ('u', 3582), ('m', 3117), ('h', 3039), ('g', 2726), ('b', 2618), ('y', 1748), ('f', 1624), ('v', 1249), ('w', 1248), ('k', 852), ('x', 365), ('q', 277), ('z', 231), ('j', 126)]\n",
      "Guessing letter: i\n",
      "Sever response: {'game_id': '01cbc242e26c', 'status': 'ongoing', 'tries_remains': 5, 'word': '_ r e _ _ _ _ a r _ '}\n",
      "len(self.n_gram_dictionary) is 22850\n",
      "len(self.n_gram_backup) is 7468580\n",
      "len(n_gram_dictionary) is 22850\n",
      "some matched grams are: \n",
      "['breviat', 'breviat', 'breviat', 'presidiary', 'dreyfusard', 'hreesquare']\n",
      "letters predicted by n_grams are: \n",
      "[('t', 8673), ('p', 8145), ('n', 7355), ('o', 6965), ('s', 6747), ('c', 6292), ('l', 5273), ('d', 4962), ('u', 3610), ('m', 3189), ('h', 3091), ('g', 2966), ('b', 2662), ('y', 1784), ('f', 1644), ('w', 1356), ('v', 1277), ('k', 888), ('x', 365), ('q', 285), ('z', 231), ('j', 130)]\n",
      "Guessing letter: t\n",
      "Sever response: {'game_id': '01cbc242e26c', 'status': 'ongoing', 'tries_remains': 4, 'word': '_ r e _ _ _ _ a r _ '}\n",
      "len(self.n_gram_dictionary) is 23638\n",
      "len(self.n_gram_backup) is 7468580\n",
      "len(n_gram_dictionary) is 23638\n",
      "some matched grams are: \n",
      "['breviat', 'breviat', 'breviat', 'presidiary', 'dreyfusard', 'hreesquare']\n",
      "letters predicted by n_grams are: \n",
      "[('p', 9105), ('n', 7579), ('o', 7149), ('s', 6867), ('c', 6492), ('d', 5386), ('l', 5289), ('u', 3666), ('g', 3446), ('m', 3333), ('h', 3195), ('b', 2750), ('y', 1856), ('f', 1684), ('w', 1572), ('v', 1333), ('k', 960), ('x', 365), ('q', 301), ('z', 231), ('j', 138)]\n",
      "Guessing letter: p\n",
      "Sever response: {'game_id': '01cbc242e26c', 'status': 'ongoing', 'tries_remains': 3, 'word': '_ r e _ _ _ _ a r _ '}\n",
      "len(self.n_gram_dictionary) is 25214\n",
      "len(self.n_gram_backup) is 7468580\n",
      "len(n_gram_dictionary) is 25214\n",
      "some matched grams are: \n",
      "['breviat', 'breviat', 'breviat', 'presidiary', 'dreyfusard', 'hreesquare']\n",
      "letters predicted by n_grams are: \n",
      "[('n', 8027), ('o', 7517), ('s', 7107), ('c', 6892), ('d', 6234), ('l', 5321), ('g', 4406), ('u', 3778), ('m', 3621), ('h', 3403), ('b', 2926), ('w', 2004), ('y', 2000), ('f', 1764), ('v', 1445), ('k', 1104), ('x', 365), ('q', 333), ('z', 231), ('j', 154)]\n",
      "Guessing letter: n\n",
      "Sever response: {'game_id': '01cbc242e26c', 'status': 'ongoing', 'tries_remains': 2, 'word': '_ r e _ _ _ _ a r _ '}\n",
      "len(self.n_gram_dictionary) is 28366\n",
      "len(self.n_gram_backup) is 7468580\n",
      "len(n_gram_dictionary) is 28366\n",
      "some matched grams are: \n",
      "['breviat', 'breviat', 'breviat', 'presidiary', 'dreyfusard', 'hreesquare']\n",
      "letters predicted by n_grams are: \n",
      "[('o', 8253), ('d', 7930), ('c', 7692), ('s', 7587), ('g', 6326), ('l', 5385), ('m', 4197), ('u', 4002), ('h', 3819), ('b', 3278), ('w', 2868), ('y', 2288), ('f', 1924), ('v', 1669), ('k', 1392), ('q', 397), ('x', 365), ('z', 231), ('j', 186)]\n",
      "Guessing letter: o\n",
      "Sever response: {'game_id': '01cbc242e26c', 'status': 'ongoing', 'tries_remains': 1, 'word': '_ r e _ _ _ _ a r _ '}\n",
      "len(self.n_gram_dictionary) is 34670\n",
      "len(self.n_gram_backup) is 7468580\n",
      "len(n_gram_dictionary) is 34670\n",
      "some matched grams are: \n",
      "['breviat', 'breviat', 'breviat', 'presidiary', 'dreyfusard', 'hreesquare']\n",
      "letters predicted by n_grams are: \n",
      "[('d', 11322), ('g', 10166), ('c', 9292), ('s', 8547), ('l', 5513), ('m', 5349), ('h', 4651), ('w', 4596), ('u', 4450), ('b', 3982), ('y', 2864), ('f', 2244), ('v', 2117), ('k', 1968), ('q', 525), ('x', 365), ('j', 250), ('z', 231)]\n",
      "Guessing letter: d\n",
      "Sever response: {'game_id': '01cbc242e26c', 'status': 'ongoing', 'tries_remains': 1, 'word': 'd r e _ _ _ _ a r d '}\n",
      "len(self.n_gram_dictionary) is 47278\n",
      "len(self.n_gram_backup) is 7468580\n",
      "len(n_gram_dictionary) is 47278\n",
      "some matched grams are: \n",
      "['dressab', 'dressee', 'dresser', 'rejeopard', 'dreyfusard', 'dreyfusard']\n",
      "letters predicted by n_grams are: \n",
      "[('g', 3551), ('w', 2163), ('s', 1816), ('u', 1365), ('h', 1262), ('b', 837), ('f', 820), ('c', 795), ('l', 755), ('m', 495), ('y', 445), ('q', 287), ('v', 266), ('k', 97), ('z', 54), ('x', 43), ('j', 32)]\n",
      "Guessing letter: g\n",
      "Sever response: {'game_id': '01cbc242e26c', 'status': 'failed', 'tries_remains': 0, 'word': 'd r e _ _ _ _ a r d '}\n",
      "Failed game: 01cbc242e26c. Because of: # of tries exceeded!\n",
      "run 1895 practice games out of an allotted 100,000. practice success rate so far = 0.485\n",
      "Playing  2  th game\n",
      "Successfully start a new game! Game ID: 0419eea890cf. # of tries remaining: 6. Word: _ _ _ _ _ _ .\n",
      "Guessing letter: e\n",
      "Sever response: {'game_id': '0419eea890cf', 'status': 'ongoing', 'tries_remains': 6, 'word': '_ e e _ e _ '}\n",
      "len(self.n_gram_dictionary) is 5330428\n",
      "len(self.n_gram_backup) is 5330428\n",
      "len(n_gram_dictionary) is 5330428\n",
      "some matched grams are: \n",
      "['aee', 'nee', 'ree', 'keenes', 'heeked', 'leeced']\n",
      "letters predicted by n_grams are: \n",
      "[('r', 75212), ('n', 48380), ('s', 44027), ('d', 41601), ('t', 36703), ('l', 36229), ('m', 20573), ('c', 17034), ('p', 16670), ('h', 14230), ('v', 13141), ('a', 12881), ('i', 10461), ('g', 9835), ('b', 9325), ('f', 6670), ('w', 6517), ('k', 5954), ('o', 4925), ('u', 4287), ('y', 3734), ('x', 3003), ('z', 2890), ('j', 1180), ('q', 410)]\n",
      "Guessing letter: r\n",
      "Sever response: {'game_id': '0419eea890cf', 'status': 'ongoing', 'tries_remains': 6, 'word': '_ e e _ e r '}\n",
      "len(self.n_gram_dictionary) is 241882\n",
      "len(self.n_gram_backup) is 5330428\n",
      "len(n_gram_dictionary) is 241882\n",
      "some matched grams are: \n",
      "['aee', 'nee', 'ree', 'heeler', 'heezer', 'reeper']\n",
      "letters predicted by n_grams are: \n",
      "[('t', 13216), ('n', 8178), ('p', 7027), ('d', 6842), ('l', 6407), ('v', 6050), ('h', 4593), ('s', 4168), ('m', 3905), ('k', 2775), ('b', 2748), ('w', 2352), ('f', 2213), ('c', 2179), ('g', 2062), ('i', 1648), ('z', 906), ('a', 891), ('y', 827), ('x', 758), ('u', 496), ('j', 258), ('o', 189), ('q', 34)]\n",
      "Guessing letter: t\n",
      "Sever response: {'game_id': '0419eea890cf', 'status': 'ongoing', 'tries_remains': 5, 'word': '_ e e _ e r '}\n",
      "len(self.n_gram_dictionary) is 86057\n",
      "len(self.n_gram_backup) is 5330428\n",
      "len(n_gram_dictionary) is 86057\n",
      "some matched grams are: \n",
      "['aee', 'nee', 'ree', 'heeler', 'heezer', 'reeper']\n",
      "letters predicted by n_grams are: \n",
      "[('n', 8178), ('p', 7027), ('d', 6842), ('l', 6407), ('v', 6050), ('h', 4593), ('s', 4168), ('m', 3905), ('k', 2775), ('b', 2748), ('w', 2352), ('f', 2213), ('c', 2179), ('g', 2062), ('i', 1648), ('z', 906), ('a', 891), ('y', 827), ('x', 758), ('u', 496), ('j', 258), ('o', 189), ('q', 34)]\n",
      "Guessing letter: n\n",
      "Sever response: {'game_id': '0419eea890cf', 'status': 'ongoing', 'tries_remains': 4, 'word': '_ e e _ e r '}\n",
      "len(self.n_gram_dictionary) is 87841\n",
      "len(self.n_gram_backup) is 5330428\n",
      "len(n_gram_dictionary) is 87841\n",
      "some matched grams are: \n",
      "['aee', 'nee', 'ree', 'heeler', 'heezer', 'reeper']\n",
      "letters predicted by n_grams are: \n",
      "[('p', 7027), ('d', 6842), ('l', 6407), ('v', 6050), ('h', 4593), ('s', 4168), ('m', 3905), ('k', 2775), ('b', 2748), ('w', 2352), ('f', 2213), ('c', 2179), ('g', 2062), ('i', 1648), ('z', 906), ('a', 891), ('y', 827), ('x', 758), ('u', 496), ('j', 258), ('o', 189), ('q', 34)]\n",
      "Guessing letter: p\n",
      "Sever response: {'game_id': '0419eea890cf', 'status': 'ongoing', 'tries_remains': 4, 'word': '_ e e p e r '}\n",
      "len(self.n_gram_dictionary) is 91577\n",
      "len(self.n_gram_backup) is 5330428\n",
      "len(n_gram_dictionary) is 91577\n",
      "some matched grams are: \n",
      "['aee', 'nee', 'ree', 'weeper', 'weeper', 'reeper']\n",
      "letters predicted by n_grams are: \n",
      "[('h', 1026), ('w', 1003), ('l', 878), ('s', 682), ('d', 625), ('k', 593), ('b', 349), ('f', 309), ('g', 209), ('m', 176), ('u', 136), ('c', 120), ('v', 65), ('z', 54), ('j', 50), ('y', 28), ('a', 18), ('o', 18), ('i', 12), ('x', 4)]\n",
      "Guessing letter: h\n",
      "Sever response: {'game_id': '0419eea890cf', 'status': 'ongoing', 'tries_remains': 3, 'word': '_ e e p e r '}\n",
      "len(self.n_gram_dictionary) is 12103\n",
      "len(self.n_gram_backup) is 5330428\n",
      "len(n_gram_dictionary) is 12103\n",
      "some matched grams are: \n",
      "['aee', 'nee', 'ree', 'weeper', 'weeper', 'reeper']\n",
      "letters predicted by n_grams are: \n",
      "[('w', 1003), ('l', 878), ('s', 682), ('d', 625), ('k', 593), ('b', 349), ('f', 309), ('g', 209), ('m', 176), ('u', 136), ('c', 120), ('v', 65), ('z', 54), ('j', 50), ('y', 28), ('a', 18), ('o', 18), ('i', 12), ('x', 4)]\n",
      "Guessing letter: w\n",
      "Sever response: {'game_id': '0419eea890cf', 'status': 'ongoing', 'tries_remains': 2, 'word': '_ e e p e r '}\n",
      "len(self.n_gram_dictionary) is 12103\n",
      "len(self.n_gram_backup) is 5330428\n",
      "len(n_gram_dictionary) is 12103\n",
      "some matched grams are: \n",
      "['aee', 'nee', 'ree', 'weeper', 'weeper', 'reeper']\n",
      "letters predicted by n_grams are: \n",
      "[('l', 878), ('s', 682), ('d', 625), ('k', 593), ('b', 349), ('f', 309), ('g', 209), ('m', 176), ('u', 136), ('c', 120), ('v', 65), ('z', 54), ('j', 50), ('y', 28), ('a', 18), ('o', 18), ('i', 12), ('x', 4)]\n",
      "Guessing letter: l\n",
      "Sever response: {'game_id': '0419eea890cf', 'status': 'ongoing', 'tries_remains': 1, 'word': '_ e e p e r '}\n",
      "len(self.n_gram_dictionary) is 12103\n",
      "len(self.n_gram_backup) is 5330428\n",
      "len(n_gram_dictionary) is 12103\n",
      "some matched grams are: \n",
      "['aee', 'nee', 'ree', 'weeper', 'weeper', 'reeper']\n",
      "letters predicted by n_grams are: \n",
      "[('s', 682), ('d', 625), ('k', 593), ('b', 349), ('f', 309), ('g', 209), ('m', 176), ('u', 136), ('c', 120), ('v', 65), ('z', 54), ('j', 50), ('y', 28), ('a', 18), ('o', 18), ('i', 12), ('x', 4)]\n",
      "Guessing letter: s\n",
      "Sever response: {'game_id': '0419eea890cf', 'status': 'failed', 'tries_remains': 0, 'word': '_ e e p e r '}\n",
      "Failed game: 0419eea890cf. Because of: # of tries exceeded!\n",
      "run 1896 practice games out of an allotted 100,000. practice success rate so far = 0.485\n"
     ]
    }
   ],
   "source": [
    "test = 2\n",
    "score = 0\n",
    "check = []\n",
    "win = 0 \n",
    "for i in range(test):\n",
    "    print('Playing ', i+1, ' th game')\n",
    "    if api.start_game(practice=1,verbose=True):\n",
    "        score += 1\n",
    "        win = 1\n",
    "    check.append([len(api.current_word),win])\n",
    "    [total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "\n",
    "    practice_success_rate = total_practice_successes / total_practice_runs\n",
    "    print('run %d practice games out of an allotted 100,000. practice success rate so far = %.3f' % (total_practice_runs, practice_success_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(score)\n",
    "print(i+1)\n",
    "print(score/(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['_ _ _ _ _ _ _ _ _ _ ', 20, 0], ['_ _ _ _ _ _ ', 12, 0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check\n",
    "# 115\n",
    "# 200\n",
    "# 0.575"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing recorded games:\n",
    "Please finalize your code prior to running the cell below. Once this code executes once successfully your submission will be finalized. Our system will not allow you to rerun any additional games.\n",
    "\n",
    "Please note that it is expected that after you successfully run this block of code that subsequent runs will result in the error message \"Your account has been deactivated\".\n",
    "\n",
    "Once you've run this section of the code your submission is complete. Please send us your source code via email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    print('Playing ', i, ' th game')\n",
    "    # Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\n",
    "    #api.start_game(practice=0,verbose=False)\n",
    "    \n",
    "    # DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To check your game statistics\n",
    "1. Simply use \"my_status\" method.\n",
    "2. Returns your total number of games, and number of wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "success_rate = total_recorded_successes/total_recorded_runs\n",
    "print('overall success rate = %.3f' % success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w': 1, 'e': 2, 'r': 3, 'c': 4, '_': 5, 'p': 6, 'h': 7, 'v': 8, 'z': 9, 'y': 10, 'l': 11, 'k': 12, 't': 13, 'a': 14, 'n': 15, 'q': 16, 'i': 17, 'm': 18, 'b': 19, 'f': 20, 's': 21, 'x': 22, 'g': 23, 'u': 24, 'o': 25, 'j': 26, 'd': 27}\n",
      "{1: 'w', 2: 'e', 3: 'r', 4: 'c', 5: '_', 6: 'p', 7: 'h', 8: 'v', 9: 'z', 10: 'y', 11: 'l', 12: 'k', 13: 't', 14: 'a', 15: 'n', 16: 'q', 17: 'i', 18: 'm', 19: 'b', 20: 'f', 21: 's', 22: 'x', 23: 'g', 24: 'u', 25: 'o', 26: 'j', 27: 'd'}\n"
     ]
    }
   ],
   "source": [
    "words = api.full_dictionary  \n",
    "X, y = model_jn.generate_training_data(words)\n",
    "\n",
    "# Convert words and labels to numerical representations\n",
    "X_numerical, y_numerical, char_to_index, index_to_char = model_jn.convert_to_numerical(X, y)\n",
    "print(char_to_index)\n",
    "print(index_to_char)\n",
    "\n",
    "# Pad sequences to ensure equal length\n",
    "X_padded = pad_sequences(X_numerical)\n",
    "y_padded = pad_sequences([y_numerical], padding='post')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1681209"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_padded.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
